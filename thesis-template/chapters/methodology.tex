\فصل{روش‌پیشنهادی}


این بخش ابتدا تقطیر ویژگی را مرور کرده و سپس جزئیات روش پیشنهادی ما را ارائه می‌دهد.


در تقطیر ویژگی، شبکه دانش‌آموز برای تقلید از نقشه‌های ویژگی شبکه معلم آموزش داده می‌شود:

\begin{equation}
L_{fd} = \ell_{\text{feat}}(\Phi_s(F_s), \Phi_t(F_t))
\end{equation}

که در آن $\ell_{\text{feat}}$ یک تابع مشابهت برای تطبیق ویژگی‌های معلم $(F_t)$ و دانش‌آموز $(F_s)$ است، و $\Phi$ یک تبدیل است که برای هم‌تراز کردن اندازه و کانال‌های ویژگی‌ها در صورت وجود ناسازگاری استفاده می‌شود.

مطالعات اخیر پیشنهاد داده‌اند که استفاده از تبدیلات پیشرفته‌تر $(\Phi_s, \Phi_t)$ نه تنها اندازه ویژگی‌ها را هم‌تراز می‌کند، بلکه به دانش‌آموز کمک می‌کند تا دانش بیشتری از معلم به دست آورد. این مسیر پژوهشی این سؤال را مطرح می‌کند که آیا طراحی تبدیلات مناسب برای یادگیری ویژگی‌های مؤثرتر توسط شبکه دانش‌آموز ضروری است؟

این پژوهش مطالعات تجربی انجام می‌دهد تا به سؤال فوق پاسخ دهد و نشان می‌دهد که مدل دانش‌آموز می‌تواند با استفاده از یک تبدیل توجهی بازنمایی‌های خود را تقویت کند. با استفاده از این بینش، ما یک روش ساده ارائه می‌دهیم که یک ماژول توجه بلوکی کانولوشنی CBAM را در ویژگی‌های دانش‌آموز و معلم ادغام می‌کند و ویژگی‌های اصلاح‌شده دانش‌آموز را با ویژگی‌های اصلاح‌شده معلم با استفاده از فاصله MSE معمولی هم‌تراز می‌کند. بخش‌های زیر ابتدا ماژول CBAM را توصیف کرده و سپس رویکرد پیشنهادی ما برای تقطیر ویژگی هدایت‌شده توسط توجه را توضیح می‌دهند. و بخش نهایی، درباره نحوه استفاده از مدل طیفی برای یافتن تمرکز بر روی تصاویر و نحوه اعمال آن بر روی ویژگی های استخراج شده از ماژول توجه توضیح می‌دهد.


فرض کنید $F \in \mathbb{R}^{c \times w \times h}$ یک نقشه ویژگی میانی به دست آمده از شبکه دانش‌آموز یا معلم باشد که ابعاد فضایی آن $h \times w$ و بعد کانالی آن $c$ است. دو ماژول توجه، توصیفگرهای توجه فضایی $(M_S(F) \in \mathbb{R}^{c \times h \times w})$ و کانالی $(M_C(F) \in \mathbb{R}^{c \times 1 \times 1})$ را از یک ویژگی میانی جمع‌آوری می‌کنند.


سپس ویژگی های کانالی و فضایی با نقشه ویژگی اصلی $F$ ضرب می‌شوند تا نقشه‌های ویژگی جدید و غنی $F'$ و $F''$ ایجاد شوند، که اطلاعات بین کانالی و بین مکانی را به نقشه ویژگی اصلی معرفی می‌کنند. فرمول کلی برای بهبود ویژگی به صورت زیر است:
\begin{equation}
F' = MC(F) \otimes F
\end{equation}
\begin{equation}
F'' = MS(F') \otimes F'
\end{equation}

که در آن $\otimes$ نشان‌دهنده ضرب عنصری است. در هنگام ضرب، نقشه‌های توجه مکانی در امتداد بعد کانال منتشر می‌شوند و نقشه‌های توجه کانالی در امتداد ابعاد مکانی منتشر می‌شوند.

برای یافتن تمرکز بر اساس موقعیت فضایی تصویر، ابتدا با استفاده از مدل طیفی، توجه تصویر را با استفاده از روشهای طیفی سنتی به دست آورده که خروجی آن قطعه‌بندی های ویژه ای هستند که تنها از قطعه های اول آنها استفاده می‌کنیم و سپس با استفاده از ماژول توجه فضایی، توجه روی تصاویر را با استفاده از میانگین گیری و ماکسیموم گیری محاسبه می‌کنیم. در نهایت، ویژگی های فضایی تصاویر به دست آمده از هر دو روش را در هر لایه از مدل تجمیع می‌کنیم که ویژگیهای نهایی فضایی ما را تشکیل می‌دهد.



ماژول توجه کانال (CAM) اطلاعات فضایی را با اعمال اپراتورهای ماکس‌پولینگ و میانگین‌پولینگ بر روی نقشه ویژگی میانه \( F \) تجمیع می‌کند. این عملیات توصیف‌گرهای متنی ایجاد می‌کند که سپس توسط یک پرسپترون چندلایه پردازش می‌شوند تا نقشه توجه کانال \( M_C(F) \) تولید شود. این نقشه نواحی معنادار تصویر را برجسته می‌کند و نواحی غیرمربوط به وظیفه بخش‌بندی مانند پس‌زمینه را محو می‌کند. توصیف‌گرهای آگاه به کانال برای نقشه ویژگی میانه \( F \) به صورت زیر تعریف می‌شوند:

\begin{equation}
M_C(F) = \sigma(W_1(W_0(F_C^{avg}))) + W_1(W_0(F_C^{max}))
\end{equation}

که در آن \( F_C^{avg} \in \mathbb{R}^{c \times 1 \times 1} \) و \( F_C^{max} \in \mathbb{R}^{c \times 1 \times 1} \) نقشه‌های ویژگی‌ای هستند که با اعمال اپراتورهای میانگین‌پولینگ و ماکس‌پولینگ به نقشه ویژگی میانه \( F \) تولید می‌شوند. \( W_1 \) و \( W_0 \) وزن‌های MLP مشترک بین دو نقشه ویژگی پول‌شده هستند. \( W_0 \) پس از یک تابع فعال‌سازی ReLU قرار می‌گیرد و \( \sigma \) نشان‌دهنده تابع سیگموئید است.



ما ابتدا ویژگی‌های عمیق \(F \in \mathbb{R}^{C \times w \times h}\) را با استفاده از یک شبکه عمیق استخراج می‌کنیم. این ویژگی‌ها می‌توانند از هر بخش شبکه استخراج شوند و حتی ترکیبی از لایه‌های مختلف باشند، یعنی هایپر-ستون‌ها \cite{hariharan2015hypercolumnsobjectsegmentationfinegrained}. در آزمایش‌های ما با ترانسفورمرها، مشابه \cite{vo2019unsupervisedimagematchingobject}، دریافتیم که ویژگی‌های کلیدهای لایه توجه آخر به‌ویژه عملکرد خوبی دارند، چرا که ذاتاً برای خود-تجمع ویژگی‌های مشابه طراحی شده‌اند.

سپس یک ماتریس وابستگی از همبستگی ویژگی‌های وصله‌محور ایجاد می‌کنیم. علاوه بر این، وابستگی‌ها را در مقدار \(0\) آستانه‌بندی می‌کنیم، زیرا ویژگی‌ها برای تجمع ویژگی‌های مشابه طراحی شده‌اند نه ویژگی‌های ضد همبسته:

\begin{equation}
W_{\text{feat}} = FF^T \odot (FF^T > 0) \in \mathbb{R}^{w \times h^2 \times w}
\end{equation}

این وابستگی‌های ویژگی اطلاعات معنایی غنی‌ای در وضوحی خشن ارائه می‌کنند. برای بازگرداندن جزئیات سطح پایین، آن‌ها را با اطلاعات سنتی در سطح رنگ که می‌توان آن را به‌عنوان ویژگی‌های لایه صفرم شبکه در نظر گرفت، ترکیب می‌کنیم.
   به‌عنوان ماتریس وابستگی رنگ، از ماتریس \(KNN\)-ماتینگ پراکنده استفاده می‌کنیم، اگرچه هر ماتریس شباهت سنتی نیز می‌تواند استفاده شود. ماتریس وابستگی نهایی ما جمع وزنی ماتریس‌های ویژگی و رنگ است:

\begin{equation}
W = W_{\text{feat}} + \lambda_{\text{knn}} W_{\text{knn}}
\end{equation}

که در آن \(\lambda_{\text{knn}}\) یک پارامتر تعریف‌شده توسط ما است که میان ثبات معنایی و رنگ تعادل برقرار می‌کند.

با داشتن \(W\)، مقادیر ویژه‌ی لاپلاسین \(L = D^{-1/2}(D - W)D^{-1/2}\) را برای تجزیه یک تصویر به بخش‌های نرم استخراج می‌کنیم: \(\{y_0, \cdots, y_{n-1}\} = \text{eigs}(L)\). از آنجا که اولین بردار ویژه \(y_0\) یک بردار ثابت متناظر با \(\lambda_0 = 0\) است، برای اهداف ما فقط از \(y > 0\) استفاده می‌کنیم. و همچنین، با توجه به دیتاست مورد استفاده، تنها به تعدادی از بخش‌بندی های ویژه به دست آمده برای آموزش مدل نیاز داریم.

در شکل تصاویر بررسی شده، دریافتیم که بخش‌های ویژه متناظر با نواحی معنایی معنادار در تصویر هستند و مرزهای دقیقی دارند. بنابراین، وظایف محلی‌سازی و بخش‌بندی به‌طور طبیعی به‌عنوان کاربردهای فوری این رویکرد ظاهر می‌شوند. نکته مهم این است که اگر انتخاب مدل استخراج‌کننده ویژگی‌ها مدلی باشد که با نظارت خودکار روی داده‌های بدون برچسب آموزش دیده است، می‌توان این وظایف را حتی بدون نظارت و بدون نیاز به تنظیم دقیق انجام داد.


ماژول توجه فضایی، مشابه به CAM عمل می‌کند. اطلاعات فضایی از نقشه ویژگی میانه \( F \) با استفاده از اپراتورهای ماکسیموم گیری و میانگین گیری تجمیع می‌شوند تا دو توصیف‌گر فضای متفاوت ایجاد شود: \( F_S^{avg} \in \mathbb{R}^{1 \times h \times w} \) و \( F_S^{max} \in \mathbb{R}^{1 \times h \times w} \). سپس توصیف‌گرهای فضایی برای \( F \) به صورت زیر محاسبه می‌شوند:

\begin{equation}
M_S(F) = \sigma(Nr(\sum_{i=1}^{k} y_i)) +  \sigma(A^{7 \times 7}([F_S^{avg}; F_S^{max}]))
\end{equation}

که در آن \( A^{7 \times 7} \) نمایانگر یک هسته کانولوشن \( 7 \times 7 \) است. Nr نمایانگر نرمالایز کننده خروجی مدل طیفی و $\sigma$ بیانگر تابع Sigmoid می‌باشد که خروجی ها را بین مقادیر 0 و 1 قرار دهیم.


‫‫\قسمت{جمع‌بندی}


در این بخش، مدلی ساده ارائه شده است که ماژول توجه بلوک‌های کانولوشنی (CBAM) را در ویژگی‌های دانش‌آموز و معلم ادغام کرده و تطابق ویژگی‌های اصلاح‌شده را با استفاده از فاصله MSE ممکن می‌سازد. این رویکرد بر اهمیت بهینه‌سازی نقشه‌های ویژگی کانال و فضا تأکید دارد و تلاش می‌کند تا اطلاعات بین‌کانالی و بین‌فضایی را تقویت کند.

همچنین، از مدل‌های طیفی برای استخراج تمرکز مکانی تصاویر استفاده شده و توجه بر بخش‌های خاصی از تصاویر اعمال می‌شود. با این روش، ویژگی‌های استخراج‌شده بهینه شده و تأثیر مدل دانش‌آموز در تقلید و یادگیری بهتر افزایش می‌یابد.