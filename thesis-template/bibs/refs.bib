
@article{ciaparrone2020deep,
	title={Deep learning in video multi-object tracking: A survey},
	author={Ciaparrone, Gioele and S{\'a}nchez, Francisco Luque and Tabik, Siham and Troiano, Luigi and Tagliaferri, Roberto and Herrera, Francisco},
	journal={Neurocomputing},
	volume={381},
	pages={61--88},
	year={2020},
	publisher={Elsevier}
}


@InProceedings{cioppa2022soccernet,
	
	title={SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos},
	
	author={Cioppa, Anthony and Giancola, Silvio and Deliege, Adrien and Kang, Le and Zhou, Xin and Cheng, Zhiyu and Ghanem, Bernard and Van Droogenbroeck, Marc},
	
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	
	pages={3491--3502},
	
	year={2022}
	
}

@misc{hinton2015distillingknowledgeneuralnetwork,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1503.02531}, 
}

@misc{romero2015fitnetshintsdeepnets,
      title={FitNets: Hints for Thin Deep Nets}, 
      author={Adriana Romero and Nicolas Ballas and Samira Ebrahimi Kahou and Antoine Chassang and Carlo Gatta and Yoshua Bengio},
      year={2015},
      eprint={1412.6550},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6550}, 
}

@misc{zagoruyko2017wideresidualnetworks,
      title={Wide Residual Networks}, 
      author={Sergey Zagoruyko and Nikos Komodakis},
      year={2017},
      eprint={1605.07146},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1605.07146}, 
}

@misc{park2019relationalknowledgedistillation,
      title={Relational Knowledge Distillation}, 
      author={Wonpyo Park and Dongju Kim and Yan Lu and Minsu Cho},
      year={2019},
      eprint={1904.05068},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1904.05068}, 
}

@misc{peng2019correlationcongruenceknowledgedistillation,
      title={Correlation Congruence for Knowledge Distillation}, 
      author={Baoyun Peng and Xiao Jin and Jiaheng Liu and Shunfeng Zhou and Yichao Wu and Yu Liu and Dongsheng Li and Zhaoning Zhang},
      year={2019},
      eprint={1904.01802},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1904.01802}, 
}

@misc{zhao2021similaritytransferknowledgedistillation,
      title={Similarity Transfer for Knowledge Distillation}, 
      author={Haoran Zhao and Kun Gong and Xin Sun and Junyu Dong and Hui Yu},
      year={2021},
      eprint={2103.10047},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.10047}, 
}


@Article{s20164616,
AUTHOR = {Park, Sangyong and Heo, Yong Seok},
TITLE = {Knowledge Distillation for Semantic Segmentation Using Channel and Spatial Correlations and Adaptive Cross Entropy},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {16},
ARTICLE-NUMBER = {4616},
URL = {https://www.mdpi.com/1424-8220/20/16/4616},
PubMedID = {32824456},
ISSN = {1424-8220},
ABSTRACT = {In this paper, we propose an efficient knowledge distillation method to train light networks using heavy networks for semantic segmentation. Most semantic segmentation networks that exhibit good accuracy are based on computationally expensive networks. These networks are not suitable for mobile applications using vision sensors, because computational resources are limited in these environments. In this view, knowledge distillation, which transfers knowledge from heavy networks acting as teachers to light networks as students, is suitable methodology. Although previous knowledge distillation approaches have been proven to improve the performance of student networks, most methods have some limitations. First, they tend to use only the spatial correlation of feature maps and ignore the relational information of their channels. Second, they can transfer false knowledge when the results of the teacher networks are not perfect. To address these two problems, we propose two loss functions: a channel and spatial correlation (CSC) loss function and an adaptive cross entropy (ACE) loss function. The former computes the full relationship of both the channel and spatial information in the feature map, and the latter adaptively exploits one-hot encodings using the ground truth labels and the probability maps predicted by the teacher network. To evaluate our method, we conduct experiments on scene parsing datasets: Cityscapes and Camvid. Our method presents significantly better performance than previous methods.},
DOI = {10.3390/s20164616}
}

@misc{mansourian2023aicsdadaptiveinterclasssimilarity,
      title={AICSD: Adaptive Inter-Class Similarity Distillation for Semantic Segmentation}, 
      author={Amir M. Mansourian and Rozhan Ahmadi and Shohreh Kasaei},
      year={2023},
      eprint={2308.04243},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.04243}, 
}

@misc{park2022prunemodeldistill,
      title={Prune Your Model Before Distill It}, 
      author={Jinhyuk Park and Albert No},
      year={2022},
      eprint={2109.14960},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2109.14960}, 
}

@misc{miles2024understandingroleprojectorknowledge,
      title={Understanding the Role of the Projector in Knowledge Distillation}, 
      author={Roy Miles and Krystian Mikolajczyk},
      year={2024},
      eprint={2303.11098},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.11098}, 
}

@misc{heo2019comprehensiveoverhaulfeaturedistillation,
      title={A Comprehensive Overhaul of Feature Distillation}, 
      author={Byeongho Heo and Jeesoo Kim and Sangdoo Yun and Hyojin Park and Nojun Kwak and Jin Young Choi},
      year={2019},
      eprint={1904.01866},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1904.01866}, 
}

@misc{yang2022crossimagerelationalknowledgedistillation,
      title={Cross-Image Relational Knowledge Distillation for Semantic Segmentation}, 
      author={Chuanguang Yang and Helong Zhou and Zhulin An and Xue Jiang and Yongjun Xu and Qian Zhang},
      year={2022},
      eprint={2204.06986},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.06986}, 
}

@InProceedings{Liu_2019_CVPR,
author = {Liu, Yifan and Chen, Ke and Liu, Chris and Qin, Zengchang and Luo, Zhenbo and Wang, Jingdong},
title = {Structured Knowledge Distillation for Semantic Segmentation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
} 

@misc{xie2018improvingfastsegmentationteacherstudent,
      title={Improving Fast Segmentation With Teacher-student Learning}, 
      author={Jiafeng Xie and Bing Shuai and Jian-Fang Hu and Jingyang Lin and Wei-Shi Zheng},
      year={2018},
      eprint={1810.08476},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1810.08476}, 
}

@article{Feng_2021,
   title={Double Similarity Distillation for Semantic Image Segmentation},
   volume={30},
   ISSN={1941-0042},
   url={http://dx.doi.org/10.1109/TIP.2021.3083113},
   DOI={10.1109/tip.2021.3083113},
   journal={IEEE Transactions on Image Processing},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Feng, Yingchao and Sun, Xian and Diao, Wenhui and Li, Jihao and Gao, Xin},
   year={2021},
   pages={5363–5376} }

@INPROCEEDINGS{8100237,

  author={Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},

  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning}, 

  year={2017},

  volume={},

  number={},

  pages={7130-7138},

  keywords={Training;Optimization;Knowledge engineering;Feature extraction;Knowledge transfer;Computer vision},

  doi={10.1109/CVPR.2017.754}}

@misc{liu2022exploringinterchannelcorrelationdiversitypreserved,
      title={Exploring Inter-Channel Correlation for Diversity-preserved KnowledgeDistillation}, 
      author={Li Liu and Qingle Huang and Sihao Lin and Hongwei Xie and Bing Wang and Xiaojun Chang and Xiaodan Liang},
      year={2022},
      eprint={2202.03680},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2202.03680}, 
}

@misc{tung2019similaritypreservingknowledgedistillation,
      title={Similarity-Preserving Knowledge Distillation}, 
      author={Frederick Tung and Greg Mori},
      year={2019},
      eprint={1907.09682},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1907.09682}, 
}

@misc{zhou2020channeldistillationchannelwiseattention,
      title={Channel Distillation: Channel-Wise Attention for Knowledge Distillation}, 
      author={Zaida Zhou and Chaoran Zhuge and Xinwei Guan and Wen Liu},
      year={2020},
      eprint={2006.01683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.01683}, 
}

@inbook{Cheeger,
url = {https://doi.org/10.1515/9781400869312-013},
title = {A Lower Bound for the Smallest Eigenvalue of the Laplacian},
booktitle = {Problems in Analysis},
author = {Jeff Cheeger},
editor = {Robert C. Gunning},
publisher = {Princeton University Press},
address = {Princeton},
pages = {195--200},
doi = {doi:10.1515/9781400869312-013},
isbn = {9781400869312},
year = {1971},
lastchecked = {2025-01-27}
}

@article{Donath1973LowerBF,
  title={Lower bounds for the partitioning of graphs},
  author={Wilm E. Donath and Alan J. Hoffman},
  journal={Ibm Journal of Research and Development},
  year={1973},
  volume={17},
  pages={420-425},
  url={https://api.semanticscholar.org/CorpusID:122138297}
}

@article{Fiedler1973,
author = {Fiedler, Miroslav},
journal = {Czechoslovak Mathematical Journal},
language = {eng},
number = {2},
pages = {298-305},
publisher = {Institute of Mathematics, Academy of Sciences of the Czech Republic},
title = {Algebraic connectivity of graphs},
url = {http://eudml.org/doc/12723},
volume = {23},
year = {1973},
}

@inproceedings{NIPS2001_801272ee,
 author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {On Spectral Clustering: Analysis and an algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf},
 volume = {14},
 year = {2001}
}

@ARTICLE{868688,

  author={Jianbo Shi and Malik, J.},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Normalized cuts and image segmentation}, 

  year={2000},

  volume={22},

  number={8},

  pages={888-905},

  keywords={Image segmentation;Brightness;Clustering algorithms;Data mining;Eigenvalues and eigenfunctions;Bayesian methods;Coherence;Tree data structures;Filling;Partitioning algorithms},

  doi={10.1109/34.868688}}


@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@INPROCEEDINGS{8578843,
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Squeeze-and-Excitation Networks}, 
  year={2018},
  volume={},
  number={},
  pages={7132-7141},
  keywords={Computer architecture;Computational modeling;Convolution;Task analysis;Convolutional codes;Adaptation models;Stacking},
  doi={10.1109/CVPR.2018.00745}}


@misc{li2019spatialgroupwiseenhanceimproving,
      title={Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks}, 
      author={Xiang Li and Xiaolin Hu and Jian Yang},
      year={2019},
      eprint={1905.09646},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1905.09646}, 
}

@inproceedings{Self_Attention,
author = {Ambartsoumian, Artaches and Popowich, Fred},
year = {2018},
month = {01},
pages = {130-139},
title = {Self-Attention: A Better Building Block for Sentiment Analysis Neural Network Classifiers},
doi = {10.18653/v1/W18-6219}
}

@misc{park2018bambottleneckattentionmodule,
      title={BAM: Bottleneck Attention Module}, 
      author={Jongchan Park and Sanghyun Woo and Joon-Young Lee and In So Kweon},
      year={2018},
      eprint={1807.06514},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1807.06514}, 
}

@misc{woo2018cbamconvolutionalblockattention,
      title={CBAM: Convolutional Block Attention Module}, 
      author={Sanghyun Woo and Jongchan Park and Joon-Young Lee and In So Kweon},
      year={2018},
      eprint={1807.06521},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1807.06521}, 
}

@inproceedings{Ouyang_2023,
   title={Efficient Multi-Scale Attention Module with Cross-Spatial Learning},
   url={http://dx.doi.org/10.1109/ICASSP49357.2023.10096516},
   DOI={10.1109/icassp49357.2023.10096516},
   booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   publisher={IEEE},
   author={Ouyang, Daliang and He, Su and Zhang, Guozhong and Luo, Mingzhu and Guo, Huaiyong and Zhan, Jian and Huang, Zhijie},
   year={2023},
   month=jun, pages={1–5} }

@ARTICLE{9678134,
  author={An, Shumin and Liao, Qingmin and Lu, Zongqing and Xue, Jing-Hao},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Efficient Semantic Segmentation via Self-Attention and Self-Distillation}, 
  year={2022},
  volume={23},
  number={9},
  pages={15256-15266},
  keywords={Semantics;Context modeling;Knowledge engineering;Correlation;Convolution;Adaptation models;Transforms;Semantic segmentation;self-attention distillation;layer-wise context distillation},
  doi={10.1109/TITS.2021.3139001}}

@INPROCEEDINGS{8953974,
  author={Fu, Jun and Liu, Jing and Tian, Haijie and Li, Yong and Bao, Yongjun and Fang, Zhiwei and Lu, Hanqing},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Dual Attention Network for Scene Segmentation}, 
  year={2019},
  volume={},
  number={},
  pages={3141-3149},
  keywords={Segmentation;Grouping and Shape;Scene Analysis and Understanding},
  doi={10.1109/CVPR.2019.00326}}

@misc{hariharan2015hypercolumnsobjectsegmentationfinegrained,
      title={Hypercolumns for Object Segmentation and Fine-grained Localization}, 
      author={Bharath Hariharan and Pablo Arbeláez and Ross Girshick and Jitendra Malik},
      year={2015},
      eprint={1411.5752},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1411.5752}, 
}

@misc{vo2019unsupervisedimagematchingobject,
      title={Unsupervised Image Matching and Object Discovery as Optimization}, 
      author={Huy V. Vo and Francis Bach and Minsu Cho and Kai Han and Yann LeCun and Patrick Perez and Jean Ponce},
      year={2019},
      eprint={1904.03148},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1904.03148}, 
}

@article{Everingham2010ThePV,
  title={The Pascal Visual Object Classes (VOC) Challenge},
  author={Mark Everingham and Luc Van Gool and Christopher K. I. Williams and John M. Winn and Andrew Zisserman},
  journal={International Journal of Computer Vision},
  year={2010},
  volume={88},
  pages={303-338},
  url={https://api.semanticscholar.org/CorpusID:4246903}
}

@misc{hinton2015distillingknowledgeneuralnetwork,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1503.02531}, 
}

@misc{yang2022crossimagerelationalknowledgedistillation,
      title={Cross-Image Relational Knowledge Distillation for Semantic Segmentation}, 
      author={Chuanguang Yang and Helong Zhou and Zhulin An and Xue Jiang and Yongjun Xu and Qian Zhang},
      year={2022},
      eprint={2204.06986},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.06986}, 
}

@misc{huang2022knowledgedistillationstrongerteacher,
      title={Knowledge Distillation from A Stronger Teacher}, 
      author={Tao Huang and Shan You and Fei Wang and Chen Qian and Chang Xu},
      year={2022},
      eprint={2205.10536},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.10536}, 
}

@article{Liu2024RethinkingKD,
  title={Rethinking Knowledge Distillation with Raw Features for Semantic Segmentation},
  author={Tao Liu and Chenshu Chen and Xi Yang and Wenming Tan},
  journal={2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2024},
  pages={1144-1153},
  url={https://api.semanticscholar.org/CorpusID:268428467}
}